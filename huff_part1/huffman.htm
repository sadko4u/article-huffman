<html>

<head>
<meta http-equiv="Content-Language" content="ru">
<meta http-equiv="Content-Type" content="text/html; charset=windows-1251">
<title>Кодирование Хафмана. Часть 1.</title>
<style>
<!--
p            { text-indent: 32; margin: 0 }
img          { position: relative; margin-top: 0; margin-bottom: 0; padding: 0 }
body         { font-size: 12pt }
-->
</style>
</head>

<body>

<h1>Кодирование Хафмана. Часть 1.</h1>

<h4><i>Автор: SadKo</i></h4>
<h3>Вступление</h3>
<p>Здравствуй, дорогой читатель! В данной статье будет рассмотрен один из
способов сжатия данных. Этот способ
является достаточно широко
распространённым и заслуживает определённого
внимания. Данный материал расчитан по
объёму на три статьи, первая из которых
будет посвяжена алгоритму сжатия, вторая -
программной реализации алгоритма, а третья -
декомпрессии. Алгоритм сжатия будет написан
на языке C++, алгоритм декомпрессии - на языке
Assembler.</p>
<p>Однако, перед тем, как приступить к
самому алгоритму, следует
включить в статю немного теории.</p>
<h3>Немного теории</h3>
<p><b>Компрессия</b> (<b>сжатие</b>) - способ
уменьшения объёма данных с целью
дальнейшей их передачи и хранения.</p>
<p><b>Декомпрессия</b> - это способ
восстановления сжатых данных в исходные.</p>
<p>Компрессия и декомпрессия могут быть как
без потери качества (когда передаваемая/хранимая
информация в сжатом виде после
декомпрессии абсолютно идентична исходной),
так и с потерей качества (когда данные после
декомпрессии отличаются от оригинальных).
Например, текстовые документы, базы данных,
программы могут быть сжаты только способом
без потери качества, в то время как картинки,
видеоролики и аудиофайлы сжимаются именно
за счёт потери качества исходных данных (характерный
пример алгоритмов - JPEG, MPEG, ADPCM), при порой
незаметной потере качества даже при сжатии
1:4 или 1:10.</p>
<p>Выделяются основные виды упаковки:</p>
<p>- <i>Десятичная упаковка</i> предназначена
для упаковки символьных данных, состоящих
только из чисел. Вместо используемых 8 бит
под символ можно вполне рационально
использовать всего лишь 4 бита для
десятичных и шестнадцатеричных цифр, 3 бита
для восьмеричных и т.д. При подобном подходе
уже ощущается сжатие минимум 1:2.</p>
<p>- <i>Относительное кодирование</i> является
кодированием с потерей качества. Оно
основано на том, что последующий элемент
данных отличается от предыдущего на
величину, занимающую в памяти меньше места,
чем сам элемент. Характерным примером
является аудиосжатие ADPCM (Adaptive Differencial Pulse Code
Modulation), широко применяемое в цифровой
телефонии и позволяющее сжимать звуковые
данные в соотношении 1:2 с практически
незаметной потерей качества.</p>
<p>- <i>Символьное подавление</i> - способ
сжатия информации, при котором длинные
последовательности из идентичных данных
заменяются более короткими.</p>
<p>- <i>Статистическое кодирование</i> основано
на том, что не все элементы данных
встречаются с одинаковой частотой (или
вероятностью). При таком подходе коды
выбираются так, чтобы наиболее часто
встречающемуся элементу соответствовал
код с наименьшей длиной, а наименее частому
- с наибольшей. Кроме этого, коды
подбираются таким образом, чтобы при
декодировании можно было однозначно
определить элмент исходных данных. При
таком подходе возможно только бит-ориентированное
кодирование, при котором выделяются
разрешённые и запрещённые коды. Если при
декодировании битовой последовательности
код оказался запрещённым, то к нему
необходимо добавить ещё один бит исходной
последовательности и повторить операцию
декодирования. Примерами такого
кодирования являются алгоритмы Шеннона и Хафмана,
последний из которых мы и будем
рассматривать.</p>

<h3>Конкретнее об алгоритме</h3>

<p>Как уже известно из предыдущего
подраздела, алгоритм Хафмана основан на
статистическом кодировании. Разберёмся
поподробнее в его реализации.</p>

<p>Пусть имеется источник данных, который
передаёт символы <img border="0" src="huffman/huff_1.wmf" width="89" height="26">
с разной степенью вероятности, то есть
каждому <img border="0" src="huffman/huff_2.wmf" width="18" height="26">
соответствует своя вероятность (или
частота) <img border="0" src="huffman/huff_3.wmf" width="45" height="26">,
при чём существует хотя бы одна пара <img border="0" src="huffman/huff_2.wmf" width="18" height="26">
 и <img border="0" src="huffman/huff_11.wmf" width="21" height="28">,<img border="0" src="huffman/huff_9.wmf" width="37" height="22">,
такие, что <img border="0" src="huffman/huff_3.wmf" width="45" height="26">
и <img border="0" src="huffman/huff_12.wmf" width="48" height="28"> не
равны. Таким образом образуется набор частот <img border="0" src="huffman/huff_4.wmf" width="164" height="26">,
при чём <img border="0" src="huffman/huff_5.wmf" width="82" height="48">,
так как передатчик не передаёт больше
никаких символов кроме как <img border="0" src="huffman/huff_1.wmf" width="89" height="26">.</p>

<p>Наша задача - подобрать такие кодовые
символы <img border="0" src="huffman/huff_6.wmf" width="84" height="26">
с длинами <img border="0" src="huffman/huff_7.wmf" width="162" height="26">,
чтобы средняя длина кодового символа не
превышала средней длины исходного символа.
При этом нужно учитывать условие, что если <img border="0" src="huffman/huff_8.wmf" width="100" height="28">
и <img border="0" src="huffman/huff_9.wmf" width="37" height="22">, то <img border="0" src="huffman/huff_10.wmf" width="98" height="28">.</p>

<p>Хафман предложил строить дерево, в
котором узлы с наибольшей вероятностью
наименее удалены от корня. Отсюда и
вытекает сам способ посторения дерева:</p>

<p>1. Выбрать два символа<img border="0" src="huffman/huff_2.wmf" width="18" height="26">
и <img border="0" src="huffman/huff_11.wmf" width="21" height="28">,<img border="0" src="huffman/huff_9.wmf" width="37" height="22">,
такие, что <img border="0" src="huffman/huff_3.wmf" width="45" height="26">
и <img border="0" src="huffman/huff_12.wmf" width="48" height="28"> из
всего списка <img border="0" src="huffman/huff_4.wmf" width="164" height="26">
являются минимальными.</p>

<p>2. Свести ветки дерева от этих двух
элементов в одну точку с вероятностью <img border="0" src="huffman/huff_13.wmf" width="125" height="28">,
пометив одну ветку нулём, а другую -
единицей (по собственному усмотрению).</p>

<p>3. Повторить пункт 1 с учётом новой точки
вместо <img border="0" src="huffman/huff_2.wmf" width="18" height="26"> и
<img border="0" src="huffman/huff_11.wmf" width="21" height="28">, если
количество получившихся точек больше единицы. В
противном случае мы достигли корня дерева.</p>

<p>Теперь попробуем воспользоваться
полученной теорией и закодировать
информацию, передаваемую источником, на
примере семи символов.</p>

<p>Разберём подробно первый цикл. На рисунке
изображена таблица, в которой каждому
символу <img border="0" src="huffman/huff_2.wmf" width="18" height="26">
соответствует своя вероятность (частота)<img border="0" src="huffman/huff_3.wmf" width="45" height="26">.
Согласно пункту 1 мы выбираем два символа из
таблицы с наименьшей вероятностью. В нашем
случае это <img border="0" src="huffman/huff_14.wmf" width="18" height="25">
и <img border="0" src="huffman/huff_15.wmf" width="20" height="25">.
Согласно пункту 2 сводим ветки дерева от <img border="0" src="huffman/huff_14.wmf" width="18" height="25">
и <img border="0" src="huffman/huff_15.wmf" width="20" height="25"> в одну
точку и помечаем ветку, ведущую к <img border="0" src="huffman/huff_14.wmf" width="18" height="25">,
единицей, а ветку, ведущую к <img border="0" src="huffman/huff_15.wmf" width="20" height="25">,-
нулём. Над новой точкой приписываем её
вероятность (в данном случае - 0.03) В дальнейшем действия повторяются
уже с учётом новой точки и без учёта <img border="0" src="huffman/huff_14.wmf" width="18" height="25">
и <img border="0" src="huffman/huff_15.wmf" width="20" height="25">.</p>

<p><img border="0" src="huffman/huff_16.wmf" width="188" height="232"></p>

<p>После многократного повторения
изложенных действий выстраивается
следующее дерево:</p>

<p><img border="0" src="huffman/huff_17.wmf" width="402" height="241"></p>

<p>По построенному дереву можно определить
значение кодов <img border="0" src="huffman/huff_6.wmf" width="84" height="26">,
осуществляя спуск от корня к
соответствующему элементу <img border="0" src="huffman/huff_2.wmf" width="18" height="26">,
при этом приписывая к получаемой
последовательности при прохождении каждой
ветки ноль или единицу (в зависимости от
того, как именуется конкретная ветка). Таким
образом таблица кодов выглядит следующим
образом:</p>

<table border="1" width="100%">
  <tr>
    <td width="10%"><img border="0" src="huffman/huff_20.wmf" width="21" height="20"></td>
    <td width="56%"><img border="0" src="huffman/huff_18.wmf" width="17" height="26"></td>
    <td width="34%"><img border="0" src="huffman/huff_19.wmf" width="42" height="26"></td>
  </tr>
  <tr>
    <td width="10%">1</td>
    <td width="56%">011111</td>
    <td width="34%">6</td>
  </tr>
  <tr>
    <td width="10%">2</td>
    <td width="56%">1</td>
    <td width="34%">1</td>
  </tr>
  <tr>
    <td width="10%">3</td>
    <td width="56%">0110</td>
    <td width="34%">4</td>
  </tr>
  <tr>
    <td width="10%">4</td>
    <td width="56%">011110</td>
    <td width="34%">6</td>
  </tr>
  <tr>
    <td width="10%">5</td>
    <td width="56%">010</td>
    <td width="34%">3</td>
  </tr>
  <tr>
    <td width="10%">6</td>
    <td width="56%">00</td>
    <td width="34%">2</td>
  </tr>
  <tr>
    <td width="10%">7</td>
    <td width="56%">01110</td>
    <td width="34%">5</td>
  </tr>
</table>
<p>Теперь попробуем закодировать
последовательность из символов.</p>

<p>Пусть символу <img border="0" src="huffman/huff_2.wmf" width="18" height="26">
соответствует (в качестве примера) число <img border="0" src="huffman/huff_20.wmf" width="21" height="20">.
Пусть имеется последовательность 12672262.
Нужно получить результирующий двоичный код.</p>

<p>Для кодирования можно использовать уже
имеющуюся таблицу кодовых символов <img border="0" src="huffman/huff_18.wmf" width="17" height="26">
при учёте, что <img border="0" src="huffman/huff_18.wmf" width="17" height="26">
соответствует символу <img border="0" src="huffman/huff_2.wmf" width="18" height="26">.
В таком случае код для цифры 1 будет
представлять собой последовательность 011111,
для цифры 2 - 1, а для цифры 6 - 00. Таким образом,
получаем следующий результат:</p>

<table border="1" width="100%">
  <tr>
    <td width="18%">Данные</td>
    <td width="63%">12672262</td>
    <td width="19%">Длина кода</td>
  </tr>
  <tr>
    <td width="18%">Исходные</td>
    <td width="63%">001 010 110 111 010 010 110 010</td>
    <td width="19%">24 бит</td>
  </tr>
  <tr>
    <td width="18%">Кодированные</td>
    <td width="63%">011111 1 00 01110 1 1 00 1</td>
    <td width="19%">19 бит</td>
  </tr>
</table>
<p>В результате кодирования мы выиграли 5 бит
и записали последовательность 19 битами
вместо 24.</p>

<p>Однако это не даёт полной оценки сжатия
данных. Вернёмся к математике и оценим
степень сжатия кода. Для этого понадобится
энтропийная оценка.</p>

<p><i>Энтропия</i> - мера неопределённости
ситуации (случайной величины) с конечным
или с чётным числом исходов. Математически
энтропия формулируется как сумма
произведений вероятностей различных
состояний системы на логарифмы этих
вероятностей, взятых с обратным знаком:</p>

<p><img border="0" src="huffman/huff_21.wmf" width="156" height="48">.</p>

<p>Где <img border="0" src="huffman/huff_22.wmf" width="21" height="20">-
прерывная случайная величина (в нашем
случае - кодовый символ), а <img border="0" src="huffman/huff_23.wmf" width="17" height="21">
- произвольное основание, большее единицы.
Выбор основания равносилен выбору
определённой единицы измерения энтропии.
Так как мы имеем дело с двоичными цифрами,
то в качестве основания рационально
выбрать <img border="0" src="huffman/huff_24.wmf" width="41" height="21">.</p>

<p>Таким образом, энтропию для нашего случая
можно представить в виде:</p>

<p><img border="0" src="huffman/huff_25.wmf" width="196" height="48">.</p>

<p>Энтропия обладает замечательным
свойством: она равна минимальной
допустимой средней длине кодового символа <img border="0" src="huffman/huff_26.wmf" width="33" height="28">
в битах. Сама же средняя длина кодового
символа вычисляется по формуле <img border="0" src="huffman/huff_27.wmf" width="152" height="48">.</p>

<p>Подставляя значения в формулы <img border="0" src="huffman/huff_28.wmf" width="41" height="24">
и <img border="0" src="huffman/huff_29.wmf" width="36" height="28">,
получаем следующий результат:</p>

<p><img border="0" src="huffman/huff_30.wmf" width="93" height="24">,</p>

<p><img border="0" src="huffman/huff_31.wmf" width="88" height="28">.</p>

<p>Величины <img border="0" src="huffman/huff_28.wmf" width="41" height="24">
и <img border="0" src="huffman/huff_29.wmf" width="36" height="28"> очень
близки, что говорит о реальном выигрыше в
выборе алгоритма. Теперь сравним среднюю
длину исходного символа и среднюю длину
кодового символа через отношение:</p>

<p><img border="0" src="huffman/huff_32.wmf" width="129" height="50">.</p>

<p>Таким образом, мы получили сжатие в
соотношении 1:1.429, что очень неплохо.</p>

<p>И напоследок, решим последнюю задачу:
дешифровка последовательности битов.</p>

<p>Пусть для нашей ситуации имеется
последовательность битов:</p>

<p>001101100001110001000111111</p>

<p>Необходимо определить исходный код, то
есть декодировать эту последовательность.</p>

<p>Конечно, в такой ситуации можно
воспользоваться таблицей кодов, но это
достаточно неудобно, так как длина кодовых
символов непостоянна. Гораздо удобнее
осуществить спуск по дереву (начиная с
корня) по следующему правилу:</p>

<p>1. Исходная точка - корень дерева.</p>

<p>2. Прочитать новый бит. Если он ноль, то
пройти по ветке, помеченной нулём, в
противном случае - единицей.</p>

<p>3. Если точка, в которую мы попали, конечная,
то мы определили кодовый символ, который
следует записать и вернуться к пункту 1. В
противном случае следует повторить пункт 2.</p>

<p>Рассмотрим пример декодирования первого
символа. Мы находимся в точке с
вероятностью 1.00 (корень дерева), считываем
первый бит последовательности и
отправляемся по ветке, помеченной нулём, в
точку с вероятностью 0.60. Так как эта точка
не является конечной в дереве, то считываем
следующий бит, который тоже равен нулю, и
отправляемся по ветке, помеченной нулём, в
точку <img border="0" src="huffman/huff_33.wmf" width="20" height="26">,
которая является конечной. Мы дешифровали
символ - это число 6. Записываем его и
возвращаемся в исходное состояние (пермещаемся
в корень).</p>

<p>Таким образом декодированная
последовательность принимает вид.</p>

<table border="1" width="100%">
  <tr>
    <td width="18%">Данные</td>
    <td width="63%">001101100001110001000111111</td>
    <td width="19%">Длина кода</td>
  </tr>
  <tr>
    <td width="18%">Кодированные</td>
    <td width="63%">00 1 1 0110 00 01110 00 1 00 011111 1</td>
    <td width="19%">27 бит</td>
  </tr>
  <tr>
    <td width="18%">Исходные </td>
    <td width="63%">62236762612</td>
    <td width="19%">33 бит</td>
  </tr>
</table>
<p>В данном случае выигрыш составил 6 бит при
достаточно небольшой длине
последовательности.</p>

<p>Вывод напрашивается сам собой: алгоритм
прост. Однако следует сделать замечание:
данный алгоритм хорош для сжатия текстовой
информации (действительно, реально мы
используем при набивке текста примерно 60
символов из доступных 256, т.е. вероятность
встретить иные символы близка к нулю), но
достаточно плох для сжатия программ (так
как в программе все символы практически
равновероятны). Так что эффективность
алгоритма очень сильно зависит от типа
сжимаемых данных.</p>

<h3>Постскриптум</h3>

<p>В этой статье мы рассмотрели алгоритм
кодирования по методу Хафмана, который
базируется на неравномерном кодировании.
Он позволяет уменьшить размер передаваемых
или хранимых данных. Алгоритм прост для
понимания и может давать реальный выигрыш.
Кроме этого, он обладает ещё одним
замечательным свойством: возможность
кодировать и декодировать информацию &quot;на
лету&quot; при условии того, что вероятности
кодовых слов правильно определены. Хотя
существует модификация алгоритма,
позволяющая изменять структуру дерева в
реальном времени.</p>

<p>В следующей статье мы рассмотрим байт-ориентированное
сжатие файлов с использованием алгоритма
Хафмана, реализованное на C++.</p>

<p>На этой ноте я с вами прощаюсь.</p>

<h3>P.P.S.</h3>

<p>Уважаемые математики! Простите меня за
возможно неправильные обозначения
переменных, констант и функций. Если
простить не сможете, то записки с угрозами
прикончить присылайте мне на e-mail.</p>

</body>

</html>
